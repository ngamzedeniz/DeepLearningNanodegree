{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL Nano - Sess 5 - CNN pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfd58hBLwnp5",
        "outputId": "dafe50fc-0ec2-4335-98e8-419183a98fd4"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "zeros = np.zeros(shape=(10,10))\n",
        "print(zeros)\n",
        "tensor_pytorch = torch.tensor(zeros)\n",
        "print(tensor_pytorch)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0nCdudXxJql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eefc51f-58de-4be4-dd85-f6efc230eab7"
      },
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "transforms.RandomHorizontalFlip(),\n",
        "transforms.RandomRotation(25),\n",
        "transforms.ToTensor(),\n",
        "transforms.Normalize((0.5), (0.5))\n",
        "])\n",
        "\n",
        "my_dataset = torchvision.datasets.MNIST(root=\".\",train=True,download=True, transform= transform)\n",
        "## PILToTensor\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(my_dataset,\n",
        "                                          batch_size=32,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=1)\n",
        "\n",
        "batch_of_tensors = next(iter(data_loader))[0].numpy()\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "#cv2_imshow()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "UceRjw323NLg",
        "outputId": "8196236c-2154-4888-d4e7-73e597e3b9c2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# helper function to un-normalize and display an image\n",
        "def imshow(img):\n",
        "    #img = img / 2 + 0.5 # unnormalize\n",
        "    plt.imshow(img) # convert from Tensor image \n",
        "\n",
        "imshow(np.squeeze(np.swapaxes(batch_of_tensors[4],0,2).T))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWqWW1XV4XmX"
      },
      "source": [
        "### Architecture\n",
        "    ### Layers \n",
        "    ### nn.Conv2d Basic necessary layer\n",
        "    ### softmax\n",
        "    ### linear\n",
        "    ### \n",
        "### Activation Functions \n",
        "### Optimizer\n",
        "### Loss\n",
        "### Training loop \n",
        "### Validation and Testing \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Hozp_O92sUa",
        "outputId": "8378a3f1-a154-42f1-eec2-a793df0af6ed"
      },
      "source": [
        "batch_of_tensors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IRuO4NvTT5P"
      },
      "source": [
        "import torchvision\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "mnist_train = torchvision.datasets.MNIST(root= \".\", train = True, download=True, transform= transform)\n",
        "mnist_test = torchvision.datasets.MNIST(root= \".\", train = False, download = True, transform= transform)\n",
        "\n",
        "# generate indices: instead of the actual data we pass in integers instead\n",
        "train_indices, val_indices, _, _ = train_test_split(\n",
        "    range(len(mnist_train)),\n",
        "    mnist_train.targets,\n",
        "    stratify=mnist_train.targets,\n",
        "    test_size=10000,\n",
        ")\n",
        "\n",
        "# generate subset based on indices\n",
        "train_split = Subset(mnist_train, train_indices)\n",
        "val_split = Subset(mnist_train, val_indices)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_split,\n",
        "                                          batch_size=32,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=1)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(val_split,\n",
        "                                          batch_size=32,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=1)\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(mnist_test,\n",
        "                                          batch_size=32,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpBEp3fqo2gJ",
        "outputId": "5a3cb063-08cb-4373-a44c-68523578b4a7"
      },
      "source": [
        "len(mnist_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQWIKUl0ozml"
      },
      "source": [
        "train_indices, val_indices, _, _ = train_test_split(\n",
        "    range(60000),\n",
        "    mnist_train.targets,\n",
        "    stratify=mnist_train.targets,\n",
        "    test_size=10000,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7BozMgipIc2",
        "outputId": "357ea6fa-31f0-4342-8e2e-9a3d61943bd7"
      },
      "source": [
        "val_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[33134,\n",
              " 13592,\n",
              " 30400,\n",
              " 7188,\n",
              " 12845,\n",
              " 21986,\n",
              " 2221,\n",
              " 43872,\n",
              " 59793,\n",
              " 135,\n",
              " 19489,\n",
              " 45708,\n",
              " 26165,\n",
              " 25199,\n",
              " 7801,\n",
              " 51075,\n",
              " 52483,\n",
              " 39347,\n",
              " 6837,\n",
              " 37248,\n",
              " 35389,\n",
              " 41286,\n",
              " 16555,\n",
              " 33583,\n",
              " 10611,\n",
              " 38104,\n",
              " 15062,\n",
              " 56554,\n",
              " 43226,\n",
              " 18131,\n",
              " 3138,\n",
              " 54203,\n",
              " 8844,\n",
              " 29568,\n",
              " 29711,\n",
              " 23255,\n",
              " 4977,\n",
              " 56929,\n",
              " 8962,\n",
              " 36855,\n",
              " 21473,\n",
              " 30865,\n",
              " 54833,\n",
              " 17400,\n",
              " 752,\n",
              " 27191,\n",
              " 25526,\n",
              " 32987,\n",
              " 11738,\n",
              " 52946,\n",
              " 57326,\n",
              " 12165,\n",
              " 2833,\n",
              " 47325,\n",
              " 4209,\n",
              " 38944,\n",
              " 14496,\n",
              " 41155,\n",
              " 32937,\n",
              " 57398,\n",
              " 32365,\n",
              " 4662,\n",
              " 18610,\n",
              " 6662,\n",
              " 20115,\n",
              " 57020,\n",
              " 43651,\n",
              " 47589,\n",
              " 40820,\n",
              " 280,\n",
              " 57540,\n",
              " 6412,\n",
              " 6557,\n",
              " 46142,\n",
              " 14817,\n",
              " 400,\n",
              " 17051,\n",
              " 28585,\n",
              " 59574,\n",
              " 56776,\n",
              " 21436,\n",
              " 34628,\n",
              " 12265,\n",
              " 40068,\n",
              " 23784,\n",
              " 45888,\n",
              " 46881,\n",
              " 29964,\n",
              " 26195,\n",
              " 42957,\n",
              " 13003,\n",
              " 7117,\n",
              " 40666,\n",
              " 3465,\n",
              " 32663,\n",
              " 20609,\n",
              " 16382,\n",
              " 6438,\n",
              " 5473,\n",
              " 56516,\n",
              " 54249,\n",
              " 14346,\n",
              " 50983,\n",
              " 26375,\n",
              " 57054,\n",
              " 5012,\n",
              " 39089,\n",
              " 8347,\n",
              " 20470,\n",
              " 43225,\n",
              " 16821,\n",
              " 41049,\n",
              " 20566,\n",
              " 41520,\n",
              " 53358,\n",
              " 50830,\n",
              " 24365,\n",
              " 11446,\n",
              " 49581,\n",
              " 9153,\n",
              " 5636,\n",
              " 1800,\n",
              " 48053,\n",
              " 34215,\n",
              " 21324,\n",
              " 35137,\n",
              " 27427,\n",
              " 26,\n",
              " 15421,\n",
              " 32424,\n",
              " 30881,\n",
              " 4704,\n",
              " 24411,\n",
              " 32981,\n",
              " 5044,\n",
              " 7900,\n",
              " 31625,\n",
              " 21781,\n",
              " 35740,\n",
              " 37716,\n",
              " 35843,\n",
              " 52465,\n",
              " 18884,\n",
              " 19081,\n",
              " 58290,\n",
              " 30923,\n",
              " 47999,\n",
              " 19343,\n",
              " 45301,\n",
              " 31308,\n",
              " 34464,\n",
              " 32413,\n",
              " 723,\n",
              " 34029,\n",
              " 13604,\n",
              " 20217,\n",
              " 55748,\n",
              " 23746,\n",
              " 1166,\n",
              " 32170,\n",
              " 29671,\n",
              " 29935,\n",
              " 24828,\n",
              " 22497,\n",
              " 35963,\n",
              " 37492,\n",
              " 41890,\n",
              " 47669,\n",
              " 30643,\n",
              " 38746,\n",
              " 2812,\n",
              " 6934,\n",
              " 40049,\n",
              " 32346,\n",
              " 38653,\n",
              " 34385,\n",
              " 14917,\n",
              " 39597,\n",
              " 28961,\n",
              " 21228,\n",
              " 56711,\n",
              " 30748,\n",
              " 21651,\n",
              " 10310,\n",
              " 42584,\n",
              " 39381,\n",
              " 6540,\n",
              " 31266,\n",
              " 12956,\n",
              " 10719,\n",
              " 20863,\n",
              " 4195,\n",
              " 53173,\n",
              " 59406,\n",
              " 28030,\n",
              " 24156,\n",
              " 38194,\n",
              " 26322,\n",
              " 2774,\n",
              " 43116,\n",
              " 21143,\n",
              " 26698,\n",
              " 59514,\n",
              " 13755,\n",
              " 54823,\n",
              " 39546,\n",
              " 42736,\n",
              " 6152,\n",
              " 44142,\n",
              " 15295,\n",
              " 17271,\n",
              " 33308,\n",
              " 10782,\n",
              " 19731,\n",
              " 8356,\n",
              " 46045,\n",
              " 5120,\n",
              " 6173,\n",
              " 6821,\n",
              " 31231,\n",
              " 15532,\n",
              " 45372,\n",
              " 32849,\n",
              " 37340,\n",
              " 34180,\n",
              " 58864,\n",
              " 20679,\n",
              " 11521,\n",
              " 9565,\n",
              " 45171,\n",
              " 25669,\n",
              " 39925,\n",
              " 24805,\n",
              " 614,\n",
              " 43896,\n",
              " 23348,\n",
              " 45908,\n",
              " 25969,\n",
              " 31241,\n",
              " 56136,\n",
              " 54300,\n",
              " 56630,\n",
              " 52829,\n",
              " 52513,\n",
              " 2875,\n",
              " 18833,\n",
              " 48252,\n",
              " 43907,\n",
              " 16233,\n",
              " 6352,\n",
              " 8676,\n",
              " 9713,\n",
              " 7256,\n",
              " 18186,\n",
              " 22659,\n",
              " 26709,\n",
              " 38269,\n",
              " 4523,\n",
              " 13231,\n",
              " 4045,\n",
              " 870,\n",
              " 9978,\n",
              " 14235,\n",
              " 9906,\n",
              " 25023,\n",
              " 11437,\n",
              " 9592,\n",
              " 40500,\n",
              " 26808,\n",
              " 23534,\n",
              " 37420,\n",
              " 16336,\n",
              " 14215,\n",
              " 7131,\n",
              " 12669,\n",
              " 49892,\n",
              " 44227,\n",
              " 1058,\n",
              " 21557,\n",
              " 32494,\n",
              " 49973,\n",
              " 57307,\n",
              " 29178,\n",
              " 21662,\n",
              " 33823,\n",
              " 57785,\n",
              " 39590,\n",
              " 41138,\n",
              " 1889,\n",
              " 14972,\n",
              " 40894,\n",
              " 57409,\n",
              " 14255,\n",
              " 52189,\n",
              " 43822,\n",
              " 58347,\n",
              " 35870,\n",
              " 10149,\n",
              " 7316,\n",
              " 44861,\n",
              " 2231,\n",
              " 52324,\n",
              " 23826,\n",
              " 5965,\n",
              " 32597,\n",
              " 15258,\n",
              " 38623,\n",
              " 48928,\n",
              " 17701,\n",
              " 23315,\n",
              " 15899,\n",
              " 16060,\n",
              " 35729,\n",
              " 28375,\n",
              " 42467,\n",
              " 19208,\n",
              " 5076,\n",
              " 18266,\n",
              " 1696,\n",
              " 54501,\n",
              " 19655,\n",
              " 8100,\n",
              " 21059,\n",
              " 45455,\n",
              " 34005,\n",
              " 27975,\n",
              " 51799,\n",
              " 24391,\n",
              " 1172,\n",
              " 15864,\n",
              " 9531,\n",
              " 42887,\n",
              " 21049,\n",
              " 32292,\n",
              " 30924,\n",
              " 15338,\n",
              " 29558,\n",
              " 24692,\n",
              " 12778,\n",
              " 14237,\n",
              " 32970,\n",
              " 16088,\n",
              " 22031,\n",
              " 220,\n",
              " 28730,\n",
              " 20596,\n",
              " 19002,\n",
              " 51757,\n",
              " 22174,\n",
              " 21659,\n",
              " 5365,\n",
              " 6307,\n",
              " 7129,\n",
              " 16771,\n",
              " 29400,\n",
              " 19209,\n",
              " 53242,\n",
              " 11745,\n",
              " 16708,\n",
              " 3592,\n",
              " 7270,\n",
              " 17938,\n",
              " 35934,\n",
              " 55933,\n",
              " 22696,\n",
              " 10203,\n",
              " 34429,\n",
              " 2978,\n",
              " 22652,\n",
              " 30836,\n",
              " 30413,\n",
              " 54484,\n",
              " 15634,\n",
              " 32333,\n",
              " 53583,\n",
              " 4433,\n",
              " 30030,\n",
              " 22363,\n",
              " 33032,\n",
              " 23730,\n",
              " 40245,\n",
              " 44888,\n",
              " 54003,\n",
              " 47828,\n",
              " 25273,\n",
              " 38922,\n",
              " 15489,\n",
              " 47353,\n",
              " 22448,\n",
              " 41034,\n",
              " 49934,\n",
              " 19466,\n",
              " 59009,\n",
              " 36516,\n",
              " 45863,\n",
              " 30804,\n",
              " 19470,\n",
              " 51097,\n",
              " 56049,\n",
              " 58563,\n",
              " 32488,\n",
              " 13106,\n",
              " 29419,\n",
              " 21442,\n",
              " 50494,\n",
              " 48319,\n",
              " 3973,\n",
              " 13486,\n",
              " 55836,\n",
              " 47502,\n",
              " 59489,\n",
              " 43918,\n",
              " 50157,\n",
              " 11709,\n",
              " 16246,\n",
              " 27974,\n",
              " 41643,\n",
              " 22945,\n",
              " 42697,\n",
              " 17865,\n",
              " 58907,\n",
              " 48320,\n",
              " 58185,\n",
              " 26070,\n",
              " 53751,\n",
              " 23237,\n",
              " 5871,\n",
              " 23805,\n",
              " 24751,\n",
              " 8587,\n",
              " 44107,\n",
              " 48417,\n",
              " 49040,\n",
              " 24126,\n",
              " 56698,\n",
              " 35261,\n",
              " 39177,\n",
              " 3438,\n",
              " 42486,\n",
              " 24257,\n",
              " 47453,\n",
              " 33354,\n",
              " 57137,\n",
              " 1176,\n",
              " 11097,\n",
              " 37151,\n",
              " 57347,\n",
              " 44982,\n",
              " 15658,\n",
              " 45312,\n",
              " 55034,\n",
              " 17213,\n",
              " 42307,\n",
              " 12826,\n",
              " 32968,\n",
              " 35791,\n",
              " 34271,\n",
              " 28912,\n",
              " 48543,\n",
              " 45143,\n",
              " 6337,\n",
              " 24559,\n",
              " 325,\n",
              " 56784,\n",
              " 56704,\n",
              " 2836,\n",
              " 4005,\n",
              " 36882,\n",
              " 249,\n",
              " 37402,\n",
              " 25272,\n",
              " 41935,\n",
              " 36627,\n",
              " 9412,\n",
              " 57743,\n",
              " 52696,\n",
              " 37293,\n",
              " 13967,\n",
              " 8437,\n",
              " 56680,\n",
              " 41617,\n",
              " 12136,\n",
              " 28722,\n",
              " 46421,\n",
              " 33517,\n",
              " 21741,\n",
              " 35709,\n",
              " 37074,\n",
              " 37625,\n",
              " 6918,\n",
              " 40265,\n",
              " 3313,\n",
              " 59164,\n",
              " 19817,\n",
              " 52179,\n",
              " 36387,\n",
              " 24920,\n",
              " 57670,\n",
              " 24240,\n",
              " 33409,\n",
              " 4715,\n",
              " 35739,\n",
              " 14311,\n",
              " 38688,\n",
              " 17966,\n",
              " 29777,\n",
              " 16252,\n",
              " 13520,\n",
              " 10443,\n",
              " 45162,\n",
              " 41136,\n",
              " 43420,\n",
              " 47195,\n",
              " 15625,\n",
              " 47065,\n",
              " 32283,\n",
              " 15360,\n",
              " 38646,\n",
              " 51363,\n",
              " 14657,\n",
              " 9365,\n",
              " 46237,\n",
              " 59429,\n",
              " 1196,\n",
              " 56947,\n",
              " 53057,\n",
              " 43109,\n",
              " 23835,\n",
              " 19457,\n",
              " 34151,\n",
              " 31993,\n",
              " 25538,\n",
              " 44249,\n",
              " 59766,\n",
              " 44251,\n",
              " 9605,\n",
              " 38255,\n",
              " 30706,\n",
              " 28819,\n",
              " 27077,\n",
              " 19511,\n",
              " 13098,\n",
              " 29023,\n",
              " 48927,\n",
              " 14729,\n",
              " 17330,\n",
              " 53868,\n",
              " 35124,\n",
              " 44405,\n",
              " 35719,\n",
              " 35166,\n",
              " 9739,\n",
              " 7116,\n",
              " 1285,\n",
              " 2022,\n",
              " 29685,\n",
              " 16782,\n",
              " 29272,\n",
              " 18390,\n",
              " 34921,\n",
              " 23396,\n",
              " 39331,\n",
              " 14342,\n",
              " 10341,\n",
              " 9193,\n",
              " 57876,\n",
              " 22608,\n",
              " 55322,\n",
              " 3645,\n",
              " 27107,\n",
              " 2623,\n",
              " 12199,\n",
              " 42087,\n",
              " 53452,\n",
              " 27078,\n",
              " 35237,\n",
              " 42829,\n",
              " 44860,\n",
              " 17634,\n",
              " 52469,\n",
              " 54233,\n",
              " 40011,\n",
              " 7195,\n",
              " 53601,\n",
              " 15551,\n",
              " 58365,\n",
              " 1220,\n",
              " 46369,\n",
              " 59106,\n",
              " 17651,\n",
              " 7304,\n",
              " 45293,\n",
              " 51073,\n",
              " 7779,\n",
              " 6101,\n",
              " 21818,\n",
              " 55046,\n",
              " 11903,\n",
              " 24371,\n",
              " 165,\n",
              " 1056,\n",
              " 26539,\n",
              " 18889,\n",
              " 42409,\n",
              " 38578,\n",
              " 1575,\n",
              " 34115,\n",
              " 30099,\n",
              " 13149,\n",
              " 7197,\n",
              " 26786,\n",
              " 583,\n",
              " 11176,\n",
              " 5101,\n",
              " 21311,\n",
              " 24282,\n",
              " 22699,\n",
              " 15208,\n",
              " 15749,\n",
              " 34723,\n",
              " 3821,\n",
              " 6739,\n",
              " 55765,\n",
              " 27841,\n",
              " 35633,\n",
              " 21328,\n",
              " 15996,\n",
              " 8984,\n",
              " 19451,\n",
              " 18628,\n",
              " 49969,\n",
              " 19737,\n",
              " 2799,\n",
              " 15312,\n",
              " 7298,\n",
              " 59860,\n",
              " 31539,\n",
              " 24191,\n",
              " 17962,\n",
              " 26654,\n",
              " 45553,\n",
              " 18360,\n",
              " 49825,\n",
              " 40151,\n",
              " 5483,\n",
              " 24150,\n",
              " 58182,\n",
              " 26188,\n",
              " 52349,\n",
              " 2756,\n",
              " 40330,\n",
              " 399,\n",
              " 16129,\n",
              " 33911,\n",
              " 55801,\n",
              " 3575,\n",
              " 48547,\n",
              " 53721,\n",
              " 22629,\n",
              " 23397,\n",
              " 19166,\n",
              " 30553,\n",
              " 9315,\n",
              " 57073,\n",
              " 21925,\n",
              " 54642,\n",
              " 30247,\n",
              " 38737,\n",
              " 25363,\n",
              " 37590,\n",
              " 13145,\n",
              " 11452,\n",
              " 44607,\n",
              " 48597,\n",
              " 34095,\n",
              " 8467,\n",
              " 24697,\n",
              " 21737,\n",
              " 14094,\n",
              " 51741,\n",
              " 33322,\n",
              " 41715,\n",
              " 44869,\n",
              " 57087,\n",
              " 879,\n",
              " 20719,\n",
              " 49793,\n",
              " 17924,\n",
              " 28133,\n",
              " 14718,\n",
              " 47328,\n",
              " 11132,\n",
              " 14702,\n",
              " 50679,\n",
              " 9321,\n",
              " 37818,\n",
              " 5837,\n",
              " 47677,\n",
              " 17683,\n",
              " 572,\n",
              " 57660,\n",
              " 17976,\n",
              " 22770,\n",
              " 14774,\n",
              " 36587,\n",
              " 8759,\n",
              " 14547,\n",
              " 3519,\n",
              " 50897,\n",
              " 3679,\n",
              " 34446,\n",
              " 43714,\n",
              " 43577,\n",
              " 9116,\n",
              " 22675,\n",
              " 28078,\n",
              " 5779,\n",
              " 44560,\n",
              " 38069,\n",
              " 52658,\n",
              " 54138,\n",
              " 43839,\n",
              " 36100,\n",
              " 52609,\n",
              " 1976,\n",
              " 9020,\n",
              " 3036,\n",
              " 54142,\n",
              " 33476,\n",
              " 18706,\n",
              " 51042,\n",
              " 39184,\n",
              " 20943,\n",
              " 20468,\n",
              " 52529,\n",
              " 8316,\n",
              " 14928,\n",
              " 15619,\n",
              " 11088,\n",
              " 29604,\n",
              " 39189,\n",
              " 48882,\n",
              " 33559,\n",
              " 40124,\n",
              " 33454,\n",
              " 49596,\n",
              " 14575,\n",
              " 37944,\n",
              " 34927,\n",
              " 26932,\n",
              " 50611,\n",
              " 15182,\n",
              " 40067,\n",
              " 18281,\n",
              " 24534,\n",
              " 20348,\n",
              " 34836,\n",
              " 38060,\n",
              " 54824,\n",
              " 56900,\n",
              " 9011,\n",
              " 55543,\n",
              " 44876,\n",
              " 17698,\n",
              " 50320,\n",
              " 40826,\n",
              " 49373,\n",
              " 14847,\n",
              " 48770,\n",
              " 41702,\n",
              " 5883,\n",
              " 11043,\n",
              " 57,\n",
              " 27590,\n",
              " 57592,\n",
              " 58363,\n",
              " 50959,\n",
              " 20007,\n",
              " 33949,\n",
              " 58713,\n",
              " 43695,\n",
              " 18068,\n",
              " 2568,\n",
              " 45859,\n",
              " 55130,\n",
              " 9693,\n",
              " 32319,\n",
              " 17342,\n",
              " 23199,\n",
              " 37604,\n",
              " 24054,\n",
              " 59052,\n",
              " 44284,\n",
              " 43246,\n",
              " 18358,\n",
              " 47510,\n",
              " 5916,\n",
              " 26964,\n",
              " 46528,\n",
              " 9113,\n",
              " 13014,\n",
              " 15438,\n",
              " 27031,\n",
              " 9230,\n",
              " 6005,\n",
              " 2989,\n",
              " 16767,\n",
              " 45200,\n",
              " 42853,\n",
              " 50774,\n",
              " 55272,\n",
              " 48660,\n",
              " 16953,\n",
              " 29317,\n",
              " 13821,\n",
              " 40505,\n",
              " 42592,\n",
              " 38479,\n",
              " 28879,\n",
              " 17279,\n",
              " 58361,\n",
              " 50604,\n",
              " 17405,\n",
              " 3458,\n",
              " 54197,\n",
              " 38478,\n",
              " 22217,\n",
              " 50154,\n",
              " 31996,\n",
              " 52518,\n",
              " 16816,\n",
              " 41428,\n",
              " 38607,\n",
              " 43026,\n",
              " 6286,\n",
              " 51974,\n",
              " 8247,\n",
              " 7677,\n",
              " 34525,\n",
              " 35136,\n",
              " 2996,\n",
              " 43288,\n",
              " 38066,\n",
              " 53611,\n",
              " 52155,\n",
              " 4631,\n",
              " 39850,\n",
              " 35315,\n",
              " 42699,\n",
              " 13126,\n",
              " 853,\n",
              " 20162,\n",
              " 50332,\n",
              " 54529,\n",
              " 53013,\n",
              " 43970,\n",
              " 19629,\n",
              " 28163,\n",
              " 27313,\n",
              " 27984,\n",
              " 28909,\n",
              " 51212,\n",
              " 38184,\n",
              " 42698,\n",
              " 56833,\n",
              " 45352,\n",
              " 52496,\n",
              " 57963,\n",
              " 56635,\n",
              " 2081,\n",
              " 44402,\n",
              " 14066,\n",
              " 19720,\n",
              " 42615,\n",
              " 42599,\n",
              " 59381,\n",
              " 37763,\n",
              " 2547,\n",
              " 29580,\n",
              " 13396,\n",
              " 8653,\n",
              " 12330,\n",
              " 11205,\n",
              " 9130,\n",
              " 52608,\n",
              " 45655,\n",
              " 25563,\n",
              " 25326,\n",
              " 1509,\n",
              " 9710,\n",
              " 58128,\n",
              " 40174,\n",
              " 52635,\n",
              " 4520,\n",
              " 32408,\n",
              " 25126,\n",
              " 6012,\n",
              " 41548,\n",
              " 56878,\n",
              " 40086,\n",
              " 18209,\n",
              " 8454,\n",
              " 5542,\n",
              " 49558,\n",
              " 34150,\n",
              " 32588,\n",
              " 14523,\n",
              " 3428,\n",
              " 6543,\n",
              " 29233,\n",
              " 49442,\n",
              " 47447,\n",
              " 17595,\n",
              " 58823,\n",
              " 39003,\n",
              " 30570,\n",
              " 23889,\n",
              " 3320,\n",
              " 29395,\n",
              " 36785,\n",
              " 31765,\n",
              " 16942,\n",
              " 29949,\n",
              " 12744,\n",
              " 57417,\n",
              " 4270,\n",
              " 56207,\n",
              " 15696,\n",
              " 26920,\n",
              " 11023,\n",
              " 2729,\n",
              " 31087,\n",
              " 17141,\n",
              " 2369,\n",
              " 21527,\n",
              " 21957,\n",
              " 4485,\n",
              " 46312,\n",
              " 14417,\n",
              " 43403,\n",
              " 4151,\n",
              " 40512,\n",
              " 18726,\n",
              " 53009,\n",
              " 42207,\n",
              " 28410,\n",
              " 19049,\n",
              " 39371,\n",
              " 3938,\n",
              " 32764,\n",
              " 19798,\n",
              " 19496,\n",
              " 26921,\n",
              " 49901,\n",
              " 3333,\n",
              " 13940,\n",
              " 9119,\n",
              " 43636,\n",
              " 29839,\n",
              " 8802,\n",
              " 41808,\n",
              " 33734,\n",
              " 4939,\n",
              " 46720,\n",
              " 15155,\n",
              " 56919,\n",
              " 47403,\n",
              " 54390,\n",
              " 23507,\n",
              " 13896,\n",
              " 15644,\n",
              " 15329,\n",
              " 8754,\n",
              " 47777,\n",
              " 27873,\n",
              " 9962,\n",
              " 23944,\n",
              " 2188,\n",
              " 1405,\n",
              " 53724,\n",
              " 27764,\n",
              " 45398,\n",
              " 42167,\n",
              " 17209,\n",
              " 3756,\n",
              " 3636,\n",
              " 41921,\n",
              " 24490,\n",
              " 36881,\n",
              " 40051,\n",
              " 44191,\n",
              " 51576,\n",
              " 27006,\n",
              " 44279,\n",
              " 3130,\n",
              " 52314,\n",
              " 5073,\n",
              " 45008,\n",
              " 30435,\n",
              " 56267,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYycdpJTzH2O"
      },
      "source": [
        "1 * 1,10 10 __10,20__  20 _____ X _____ 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G20OEv20zW22"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVKu7f7Yesre",
        "outputId": "38528486-414a-4a60-f44b-9c6114459457"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "# define the CNN architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # convolutional layer\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "\n",
        "        # max pooling layer\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.pool3 = nn.MaxPool2d(4, 4)\n",
        "\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear1 = nn.Linear(64,32)\n",
        "        self.linear2 = nn.Linear(32, 32)\n",
        "        self.linear3 = nn.Linear(32,10)\n",
        "\n",
        "        #self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # add sequence of convolutional and max pooling layers\n",
        "\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = self.pool3(F.relu(self.conv3(x)))\n",
        "\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.linear1(x)\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = self.linear3(x)\n",
        "        x = F.softmax(x)\n",
        "        return x\n",
        "\n",
        "# create a complete CNN\n",
        "model = Net()\n",
        "print(model)\n",
        "\n",
        "# move tensors to GPU if CUDA is available\n",
        "if train_on_gpu:\n",
        "    model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (pool3): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
            "  (softmax): Softmax(dim=None)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "  (linear3): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI3yHZ88wkuw",
        "outputId": "805bf54b-8e6e-4355-9a46-f26632fd8bb2"
      },
      "source": [
        "# Thanks to https://stackoverflow.com/questions/56450969/how-to-calculate-output-sizes-after-a-convolution-layer-in-a-configuration-file/56452756\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import functools\n",
        "import operator\n",
        "\n",
        "def shape_of_output(shape_of_input, list_of_layers):\n",
        "    sequential = nn.Sequential(*list_of_layers)\n",
        "    return tuple(sequential(torch.rand(1, *shape_of_input)).shape)\n",
        "\n",
        "def size_of_output(shape_of_input, list_of_layers):\n",
        "    return functools.reduce(operator.mul, list(shape_of_output(shape_of_input, list_of_layers)))\n",
        "\n",
        "import random\n",
        "out_channel_of_first = random.randint(1,16)\n",
        "kernel_size_of_first = random.choice([3,5,7,11])\n",
        "grayscale_image_shape = (1, 28, 28)\n",
        "color_image_shape     = (3, 48, 48) # alternative example\n",
        "\n",
        "# \n",
        "# example usage\n",
        "# \n",
        "print('the output shape will be', shape_of_output(\n",
        "    shape_of_input=grayscale_image_shape,\n",
        "    list_of_layers=[         \n",
        "        nn.Conv2d(1, 16, 3, padding=1),\n",
        "        nn.MaxPool2d(2, 2),\n",
        "        nn.Conv2d(16, 32, 3, padding=1),\n",
        "        nn.MaxPool2d(2, 2),\n",
        "        nn.Conv2d(32, 64, 3, padding=1),\n",
        "        nn.MaxPool2d(4, 4),\n",
        "\n",
        "        nn.Flatten()\n",
        "    ],\n",
        "))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the output shape will be (1, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYYlWgcuWdg4",
        "outputId": "aaffc4b2-2064-4341-e24e-57f52e115033"
      },
      "source": [
        "!pip install torchinfo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.5.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au4RH3tv-OFg",
        "outputId": "590e1177-2af0-438b-d723-be342923523f"
      },
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "summary(model, input_size=(32, 1, 28, 28))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Net                                      --                        --\n",
              "├─Conv2d: 1-1                            [32, 16, 28, 28]          160\n",
              "├─MaxPool2d: 1-2                         [32, 16, 14, 14]          --\n",
              "├─Conv2d: 1-3                            [32, 32, 14, 14]          4,640\n",
              "├─MaxPool2d: 1-4                         [32, 32, 7, 7]            --\n",
              "├─Conv2d: 1-5                            [32, 64, 7, 7]            18,496\n",
              "├─MaxPool2d: 1-6                         [32, 64, 1, 1]            --\n",
              "├─Flatten: 1-7                           [32, 64]                  --\n",
              "├─Linear: 1-8                            [32, 128]                 8,320\n",
              "├─Linear: 1-9                            [32, 256]                 33,024\n",
              "├─Linear: 1-10                           [32, 10]                  2,570\n",
              "==========================================================================================\n",
              "Total params: 67,210\n",
              "Trainable params: 67,210\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 63.52\n",
              "==========================================================================================\n",
              "Input size (MB): 0.10\n",
              "Forward/backward pass size (MB): 5.72\n",
              "Params size (MB): 0.27\n",
              "Estimated Total Size (MB): 6.09\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "K3qurbIIrKUi",
        "outputId": "5bc182f0-fa1e-4fe3-9946-010f5f147738"
      },
      "source": [
        "1,256 ** (256,10)  = 1,10\n",
        "2560 + 10 = 2570"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-04a1388dd4e8>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    1,256 ** (256,10)  = 1,10\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I1bg4uoezUE"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# specify loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# specify optimizer\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCQ0-oB1e31d",
        "outputId": "d5c66a89-c374-47d0-c6a4-3bb222ae4144"
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 10 # you may increase this number to train a final model\n",
        "\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "        \n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average validation loss \n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "    \n",
        "    # calculate average losses\n",
        "    train_loss = train_loss/len(train_loader.dataset)\n",
        "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
        "        \n",
        "    # print training/validation statistics \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch, train_loss, valid_loss))\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
        "        valid_loss_min = valid_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 1.730904 \tValidation Loss: 1.614041\n",
            "Validation loss decreased (inf --> 1.614041).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 1.584629 \tValidation Loss: 1.594939\n",
            "Validation loss decreased (1.614041 --> 1.594939).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 1.564550 \tValidation Loss: 1.558665\n",
            "Validation loss decreased (1.594939 --> 1.558665).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 1.564021 \tValidation Loss: 1.548006\n",
            "Validation loss decreased (1.558665 --> 1.548006).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 1.556932 \tValidation Loss: 1.554451\n",
            "Epoch: 6 \tTraining Loss: 1.559897 \tValidation Loss: 1.570064\n",
            "Epoch: 7 \tTraining Loss: 1.574370 \tValidation Loss: 1.593158\n",
            "Epoch: 8 \tTraining Loss: 1.569305 \tValidation Loss: 1.550008\n",
            "Epoch: 9 \tTraining Loss: 1.575609 \tValidation Loss: 1.574688\n",
            "Epoch: 10 \tTraining Loss: 1.580753 \tValidation Loss: 1.573393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kddB7iCf-__j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}